# Local LLM Configuration (optional)
# For local models like llama.cpp, Ollama, etc.
LLM_API_ENDPOINT=http://localhost:8080/v1/completions
LLM_API_KEY=optional_key_if_needed

# Alternative: Use cloud LLM providers
# OPENAI_API_KEY=your_openai_key_here
# ANTHROPIC_API_KEY=your_anthropic_key_here

# Model Paths
MLP_MODEL_PATH=models/mlp_classifier.h5
TFIDF_VECTORIZER_PATH=models/tfidf_vectorizer.pkl

# App Config
DEBUG=True
LOG_LEVEL=INFO
