# Ollama Setup - Instrukcja

Ollama to narzÄ™dzie do uruchamiania duÅ¼ych modeli jÄ™zykowych (LLM) lokalnie na Twoim komputerze.

## Opcja A: Docker (ZALECANE) ğŸ³

### 1. Uruchom Ollama w Dockerze
```bash
# Uruchom wszystkie serwisy (Qdrant + Ollama)
docker-compose up -d

# SprawdÅº czy dziaÅ‚a
docker ps
# Powinny byÄ‡ 2 kontenery: medisage-qdrant i medisage-ollama
```

### 2. Pobierz model Mistral do kontenera
```bash
# WejdÅº do kontenera Ollama
docker exec -it medisage-ollama ollama pull mistral
```

To moÅ¼e zajÄ…Ä‡ kilka minut (model ma ~4GB). Model zostanie zapisany w `./ollama_models/` i bÄ™dzie persystentny.

Inne dostÄ™pne modele:
- `llama2` - Meta's Llama 2 (7B)
- `phi` - Microsoft Phi (2.7B, szybszy, mniej pamiÄ™ci)
- `mistral` - Mistral 7B (zalecany, dobry balans)
- `gemma` - Google Gemma

### 3. Zweryfikuj Å¼e dziaÅ‚a
```bash
curl http://localhost:11434/api/tags
```

JeÅ›li zobaczysz listÄ™ modeli w JSON - wszystko dziaÅ‚a!

### 4. Przydatne komendy Docker

```bash
# Zatrzymaj wszystkie serwisy
docker-compose down

# Uruchom ponownie
docker-compose up -d

# Zobacz logi Ollama
docker logs medisage-ollama

# WejdÅº do kontenera i uruchom model interaktywnie
docker exec -it medisage-ollama ollama run mistral

# Lista pobranych modeli
docker exec -it medisage-ollama ollama list

# UsuÅ„ model (jeÅ›li chcesz zaoszczÄ™dziÄ‡ miejsce)
docker exec -it medisage-ollama ollama rm mistral
```

---

## Opcja B: Natywna instalacja (alternatywa)

JeÅ›li wolisz zainstalowaÄ‡ Ollama natywnie zamiast Dockera:

### macOS / Linux
```bash
# Pobierz z https://ollama.ai
# Lub na macOS:
brew install ollama
```

### Windows
Pobierz instalator z: https://ollama.ai/download

### Uruchomienie
```bash
# Pobierz model
ollama pull mistral

# Uruchom (automatycznie jako serwis)
ollama run mistral

# SprawdÅº status
curl http://localhost:11434/api/tags
```

## UÅ¼ycie w MediSage

Po uruchomieniu Ollama:
1. Uruchom aplikacjÄ™: `streamlit run app.py`
2. W sidebar powinien pojawiÄ‡ siÄ™ status: "âœ… Ollama: dziaÅ‚a"
3. Zadaj pytanie - odpowiedÅº bÄ™dzie wygenerowana przez lokalny model

## Zmiana modelu

W pliku `app.py` zmieÅ„ liniÄ™:
```python
OLLAMA_MODEL = "mistral"  # ZmieÅ„ na "llama2", "phi", etc.
```

## Troubleshooting

### Ollama nie uruchamia siÄ™
```bash
# Uruchom rÄ™cznie:
ollama serve
```

### Model generuje wolno
- UÅ¼yj mniejszego modelu (phi zamiast mistral)
- Upewnij siÄ™ Å¼e masz wystarczajÄ…co RAM (minimum 8GB)

### BÅ‚Ä…d "model not found"
```bash
# Pobierz model ponownie:
ollama pull mistral
```

## Przydatne komendy

```bash
# Lista pobranych modeli
ollama list

# UsuÅ„ model
ollama rm mistral

# SprawdÅº logi
ollama logs
```
